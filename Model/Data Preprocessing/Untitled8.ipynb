{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-zM7bh38cve"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "try:\n",
        "  import einops\n",
        "  from einops import rearrange,reduce,repeat\n",
        "except ImportError:\n",
        "  os.system('pip install einops')\n",
        "  from einops import rearrange,reduce,repeat\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "  def __init__(self,dim,attention_dropout,num_heads):\n",
        "    super().__init__()\n",
        "    self.dim=dim\n",
        "    self.attention_dropout=attention_dropout\n",
        "    self.num_heads=num_heads\n",
        "\n",
        "    self.q=nn.Linear(dim,dim)\n",
        "    self.k=nn.Linear(dim,dim)\n",
        "    self.v=nn.Linear(dim,dim)\n",
        "    self.out=nn.Linear(dim,dim)\n",
        "  def forward(self,x,position_emb,is_casual=False):\n",
        "    H=self.num_heads\n",
        "    #B T D\n",
        "    assert position_emb is not None\n",
        "\n",
        "    q=rearrange(self.q(x + position_emb),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "    k=rearrange(self.k(x + position_emb),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "    v=rearrange(self.v(x),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "\n",
        "    attn=F.scaled_dot_product_attention(q,k,v,is_causal=is_casual)\n",
        "    attn=rearrange(tensor=attn,pattern=\"B H T D -> B T (H D)\")\n",
        "    attn=self.out(attn)\n",
        "\n",
        "    return attn\n"
      ],
      "metadata": {
        "id": "iOuBMT9V8k3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cross_Attn(nn.Module):\n",
        "  def __init__(self,dim,attention_dropout,num_heads):\n",
        "    super().__init__()\n",
        "    self.dim=dim\n",
        "    self.attention_dropout=attention_dropout\n",
        "    self.num_heads=num_heads\n",
        "\n",
        "    self.k=nn.Linear(dim,dim)\n",
        "    self.v=nn.Linear(dim,dim)\n",
        "    self.q=nn.Linear(dim,dim)\n",
        "    self.out=nn.Linear(dim,dim)\n",
        "\n",
        "  def forward(self,kv,q,q_embedding,k_embedding):\n",
        "    H=self.num_heads\n",
        "    k=rearrange(tensor=self.k(kv + k_embedding),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "\n",
        "    v=rearrange(tensor=self.v(kv),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "\n",
        "    q=rearrange(tensor=self.q(q + q_embedding),pattern=\"B T (D H) -> B H T D\",H=H)\n",
        "\n",
        "    attn=F.scaled_dot_product_attention(q,k,v,is_causal=False)\n",
        "    attn=rearrange(tensor=attn,pattern=\"B H T D -> B T (H D)\")\n",
        "    attn=self.out(attn)\n",
        "\n",
        "    return attn\n",
        "\n"
      ],
      "metadata": {
        "id": "Xd5ejise-X3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,dim):\n",
        "    super().__init__()\n",
        "    self.dim=dim\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(dim,dim*2),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(dim*2,dim)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "hfyS_frLD2f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Add_Norm(nn.Module):\n",
        "  def __init__(self,module,dim):\n",
        "    super().__init__()\n",
        "    self.module=module\n",
        "    self.dim=dim\n",
        "    self.ln=nn.LayerNorm(dim)\n",
        "\n",
        "  def forward(self,x,*args,**kwargs):\n",
        "    return x + self.module(self.ln(x),*args,**kwargs)"
      ],
      "metadata": {
        "id": "CjNCxrOgR9Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_layer(nn.Module):\n",
        "  def __init__(self,dim,n_heads,attn_drop=0.):\n",
        "    super().__init__()\n",
        "    self.MHA=Add_Norm(MHA(dim,attn_drop,num_heads=n_heads),dim)\n",
        "    self.ffn=Add_Norm(MLP(dim),dim)\n",
        "\n",
        "  def forward(self,x,position_emb):\n",
        "    x=self.MHA(x,position_emb)\n",
        "    x=self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xXwemizNR8OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_layer(nn.Module):\n",
        "  def __init__(self,dim,n_heads,attn_drop=0.,first=False):\n",
        "    super().__init__()\n",
        "    self.MMHA= nn.Identity() if first else Add_Norm(MHA(dim,attn_drop,n_heads),dim)\n",
        "    self.cross_attn=Add_Norm(Cross_Attn(dim,attn_drop,n_heads),dim)\n",
        "    self.ffn=Add_Norm(MLP(dim),dim)\n",
        "\n",
        "  def forward(self,dec_input,enc_input,\n",
        "              q_embedding,k_embedding,k_d_embedding):\n",
        "\n",
        "    dec_out=self.MMHA(dec_input,q_embedding,True)\n",
        "    mlp_out=self.cross_attn(enc_input,q=dec_out,q_embedding=q_embedding,k_embedding=k_embedding)\n",
        "    out=self.ffn(mlp_out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "jIRfsyrCVWlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sepsis_Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self,n_heads=4,n_encoder_layers=2,n_decoder_layers=2,attn_drop=0.,dim=128,feature_count=1,mean=False,num_classes=2):\n",
        "    super().__init__()\n",
        "    self.n_heads=n_heads\n",
        "    self.n_encoder_layers=n_encoder_layers\n",
        "    self.n_decoder_layers=n_decoder_layers\n",
        "    self.attn_drop=attn_drop\n",
        "    self.mean=False\n",
        "    self.learnable_query=nn.Parameter(torch.zeros(feature_count,dim),requires_grad=True)\n",
        "\n",
        "    self.position_embedding=nn.Parameter(torch.randn(feature_count,dim),requires_grad=True)\n",
        "    self.encoder_network=nn.ModuleList([\n",
        "        Encoder_layer(dim,n_heads) for _ in range(n_encoder_layers)\n",
        "    ])\n",
        "\n",
        "    self.decoder_network=nn.ModuleList([\n",
        "        Decoder_layer(dim,n_heads) for _ in range(n_decoder_layers)\n",
        "    ])\n",
        "\n",
        "    self.classification= nn.Sequential(\n",
        "        nn.Linear(dim,num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,D=x.shape\n",
        "    for i in range(self.n_encoder_layers):\n",
        "\n",
        "      x=self.encoder_network[i](x,position_emb=self.position_embedding)\n",
        "\n",
        "    encoder_out=x\n",
        "    dec_in=repeat(self.learnable_query,pattern=\"T D -> B T D\",B=B)\n",
        "    for j in range(self.n_decoder_layers):\n",
        "      \"\"\"self,dec_input,enc_input,\n",
        "              q_embedding,k_embedding,k_d_embedding\"\"\"\n",
        "      dec_in=self.decoder_network[i](dec_input=dec_in,enc_input=encoder_out,\n",
        "                                     q_embedding=dec_in,k_d_embedding=dec_in,k_embedding=self.position_embedding)\n",
        "\n",
        "    final=dec_in[:,-1,:]\n"
      ],
      "metadata": {
        "id": "zzMwHlXvFByp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features=18\n",
        "model=Sepsis_Transformer(feature_count=Features)"
      ],
      "metadata": {
        "id": "fo6goQSrzdzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.randn(1,Features,128)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm99vVQy029e",
        "outputId": "42b232d7-36d6-4163-922d-a6ea72e6be3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 18, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dcvszFo1wOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}